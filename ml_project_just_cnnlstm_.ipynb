{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_project_just_cnnlstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHN15VXUQh-l"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path, PureWindowsPath\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Reshape, Dropout, GRU, LSTM, Conv3D, ConvLSTM2D, MaxPooling3D, MaxPooling2D, BatchNormalization, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7rEM91qp3rF"
      },
      "source": [
        "#reset Keras Session\n",
        "def reset_keras():\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "    sess.close()\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "\n",
        "    try:\n",
        "        del classifier # this is from global space - change this as you need\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1G82GuO-tez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c629a178-c0ab-4eb3-970c-87085c36e46b"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f1d006-1d3e-4f76-f365-0c81ea437570"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec  8 05:45:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y_YbPH0sP8s"
      },
      "source": [
        "!unzip dev_dataset_csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukTP5162QnUv"
      },
      "source": [
        "def load_beam_visual_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    image_cols = [\"Img Path 1\", \"Img Path 2\", \"Img Path 3\", \"Img Path 4\", \"Img Path 5\", \"Img Path 6\", \"Img Path 7\", \"Img Path 8\"]\n",
        "    beam_cols = [\"Beam 1\", \"Beam 2\", \"Beam 3\", \"Beam 4\", \"Beam 5\", \"Beam 6\", \"Beam 7\", \"Beam 8\"]\n",
        "    target_cols = [\"Beam 9\"]\n",
        "    target = df[target_cols].to_numpy()\n",
        "\n",
        "    dev_path = \"./dev_dataset_csv/\"\n",
        "    images_path = df[image_cols].to_numpy()\n",
        "    image_features = np.array([Path(dev_path, PureWindowsPath(im_path)) for im_path in images_path.reshape(-1,)]).reshape(images_path.shape)\n",
        "    beam_features = df[beam_cols].to_numpy()\n",
        "    # image_path = train_dataset.iloc[0, :][\"Img Path 1\"]\n",
        "    # need to declare the image path as Windows before converting it to a Unix path\n",
        "    return np.hstack((beam_features, image_features)), target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P-9LS_qR6Wi"
      },
      "source": [
        "# Create a method that returns the second model (CNN+LSTM) using Keras APIs\n",
        "def build_cnn_model(hist_size, image_shape, codebook_size, num_kernels=40, cnn_layers=3, dropout_rate=0.2):\n",
        "\n",
        "    input_cnn = Input(shape=(hist_size, image_shape[0], image_shape[1], image_shape[2]), name=\"input_cnn_lstm\")\n",
        "\n",
        "    # CNN+LSTM part of the network (spatio-temporal features extraction from a sequence of images)\n",
        "    layer_out_cnn = input_cnn\n",
        "    for i in range(cnn_layers):\n",
        "        layer_out_cnn = ConvLSTM2D(num_kernels, strides=(1,1), kernel_size=(3,3), return_sequences=True, data_format=\"channels_last\",\n",
        "                                   dropout=dropout_rate, name=\"cnn_layer_\"+str(i+1))(layer_out_cnn)\n",
        "        layer_out_cnn = MaxPooling3D(pool_size=(1,2,2))(layer_out_cnn)\n",
        "        layer_out_cnn = BatchNormalization()(layer_out_cnn)\n",
        "    layer_out_cnn = Flatten()(layer_out_cnn)\n",
        "\n",
        "    dense_out = Dense(codebook_size, activation='softmax')(layer_out_cnn)\n",
        "\n",
        "    model = Model(inputs=input_cnn, outputs=dense_out)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVLxbeLwzYEP",
        "outputId": "0e7d1df9-f38a-4f0d-d3bf-4eabfc819e89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B91LNqK1SC8I"
      },
      "source": [
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, y, hist_size, im_size, batch_size=32, shuffle=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tot_data = self.X.shape[0]\n",
        "        self.num_classes = self.y.shape[1]\n",
        "        self.hist_size = hist_size\n",
        "        self.im_size = im_size\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        # returns the number of steps per epoch\n",
        "        return self.tot_data // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # returns one batch of data\n",
        "        indexes = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        return self.__getdata__(indexes)\n",
        "\n",
        "    def __getdata__(self, indexes):\n",
        "        image_batch = []\n",
        "        y_batch = []\n",
        "        scaling = tf.constant([255], dtype=tf.float16)\n",
        "        for i,idx in enumerate(indexes):\n",
        "            y_batch.append(tf.convert_to_tensor(self.y[idx,:], dtype=tf.uint8))\n",
        "            h_image_b = []\n",
        "            for j in range(self.hist_size):\n",
        "                img = tf.image.decode_jpeg(tf.io.read_file(tf.compat.path_to_str(self.X[idx, self.hist_size+j])))\n",
        "                img = tf.image.resize(img, [self.im_size[0], self.im_size[1]])\n",
        "                img = tf.image.convert_image_dtype(img, dtype=tf.float16, saturate=False)\n",
        "                h_image_b.append(tf.divide(img, scaling))\n",
        "            image_batch.append(h_image_b)\n",
        "        return tf.stack(image_batch), tf.stack(y_batch)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.index = np.arange(self.tot_data)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSvWSAmnVL_D",
        "outputId": "a15e3fef-faee-45a5-9c3e-924cee8b5349"
      },
      "source": [
        "train_path = \"./dev_dataset_csv/train_set.csv\"\n",
        "val_path = \"./dev_dataset_csv/val_set.csv\"\n",
        "# test_path = \"./viwi_bt_testset_csv_format_eval/testset_evaluation.csv\"\n",
        "Xtr, ytr = load_beam_visual_data(train_path)\n",
        "Xval, yval = load_beam_visual_data(val_path)\n",
        "# Xts, yts = load_beam_data(test_path) # Test data is formatted in a differemt way, need to modify the loader\n",
        "\n",
        "print(f\"Training data shape: {Xtr.shape}\")\n",
        "print(f\"Validation data shape: {Xval.shape}\")\n",
        "# print(f\"Test data shape: {Xts.shape}\")\n",
        "# One-hot-encoding of training and val target\n",
        "enc = OneHotEncoder()\n",
        "enc.fit_transform(np.vstack((ytr, [0]))) # needed to manually add codeword \"0\" in order to one-hot-code to the correct codebook size\n",
        "# It seems codeword corresponding to index 0 has not been collected in the data\n",
        "ytr_e = enc.transform(ytr).toarray()\n",
        "yval_e = enc.transform(yval).toarray()\n",
        "# yts_e = enc.transform(yts).toarray()\n",
        "print(f\"Encoded training target shape: {ytr_e.shape}\")\n",
        "print(f\"Encoded validation target shape: {yval_e.shape}\")\n",
        "# print(f\"Encoded test target shape: {yts_e.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape: (281100, 16)\n",
            "Validation data shape: (120468, 16)\n",
            "Encoded training target shape: (281100, 128)\n",
            "Encoded validation target shape: (120468, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC0Y2r15SJN4",
        "outputId": "edd3f77d-6325-4fb3-f4b7-a13ba927f5af"
      },
      "source": [
        "# create the model, print a summary to check all the parameters\n",
        "# K.clear_session()\n",
        "reset_keras()\n",
        "hist_size = Xtr.shape[1]//2\n",
        "codebook_size = ytr_e.shape[1]\n",
        "print(codebook_size)\n",
        "target_im_size = (1280//5,720//5,3)\n",
        "\n",
        "n_kernels=10\n",
        "cnn_layers = 5\n",
        "model = build_cnn_model(hist_size, target_im_size, int(codebook_size), \n",
        "                        num_kernels=n_kernels, cnn_layers=cnn_layers)\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model with proper optimizer (Adam(lr=0.001)) and loss function \n",
        "init_lr = 1e-3\n",
        "opt = Adam(lr=init_lr, amsgrad=True)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# define the following callbacks:\n",
        "# - model_checkpoint: https://keras.io/api/callbacks/model_checkpoint/ (read doc and understand its function)\n",
        "'''\n",
        "def scheduler(epoch, lr):\n",
        "    if (epoch+1) % lr_update_step == 0:\n",
        "        print(f\"Upating learning rate at epoch: {epoch}; new lr: {lr*lr_decay}\")\n",
        "        return lr*lr_decay\n",
        "    else:\n",
        "        return lr        \n",
        "lr_callback = LearningRateScheduler(scheduler)\n",
        "'''\n",
        "\n",
        "model_path = \"./drive/MyDrive/model-just-cnn-{epoch:02d}.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(model_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
        "\n",
        "# test saving \n",
        "test_path = \"./drive/MyDrive/model_test\"\n",
        "model.save(test_path, save_format='h5')\n",
        "\n",
        "n_epochs = 50\n",
        "tr_batch_size = 32\n",
        "val_batch_size = 50\n",
        "# Creates Training and Validation data generators\n",
        "train_generator = CustomDataGenerator(Xtr, ytr_e, hist_size, target_im_size, batch_size=tr_batch_size)\n",
        "val_generator = CustomDataGenerator(Xval, yval_e, hist_size, target_im_size, batch_size=val_batch_size)\n",
        "Xval_image, yval_gen = val_generator.__getitem__(0)\n",
        "\n",
        "# fit model on train data using batch_size and epochs as in paper [1]. Use also the callbacks you defined.\n",
        "# https://keras.io/api/models/model_training_apis/\n",
        "'''n_steps = Xtr.shape[0] // tr_batch_size\n",
        "for ep in range(n_epochs):\n",
        "    print(f\"Starting epoch: {ep}\")\n",
        "    for step in range(n_steps):\n",
        "        print(f\"Step: {step}\")\n",
        "        [Xtrain_beam, Xtrain_im], ytrain = train_generator.__getitem__(step)\n",
        "        results = model.train_on_batch([Xtrain_beam, Xtrain_im], ytrain)\n",
        "    train_generator.on_epoch_end()\n",
        "    print(f\"Completed epoch: {ep}\")'''\n",
        "\n",
        "hist = model.fit(train_generator, validation_data=(Xval_image, yval_gen), epochs=n_epochs, callbacks=[model_checkpoint],\n",
        "                 workers=12)\n",
        "# hist = model.fit(train_generator, epochs=n_epochs)\n",
        "\n",
        "# plot training statistics. \n",
        "pickle.dump(hist.history, open( \"history.p\", \"wb\" ))\n",
        "\n",
        "# evaluate model on test data. print the accuracy "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_cnn_lstm (InputLayer)  [(None, 8, 256, 144, 3)]  0         \n",
            "_________________________________________________________________\n",
            "cnn_layer_1 (ConvLSTM2D)     (None, 8, 254, 142, 10)   4720      \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 8, 127, 71, 10)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 127, 71, 10)    40        \n",
            "_________________________________________________________________\n",
            "cnn_layer_2 (ConvLSTM2D)     (None, 8, 125, 69, 10)    7240      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 8, 62, 34, 10)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 62, 34, 10)     40        \n",
            "_________________________________________________________________\n",
            "cnn_layer_3 (ConvLSTM2D)     (None, 8, 60, 32, 10)     7240      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 8, 30, 16, 10)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 30, 16, 10)     40        \n",
            "_________________________________________________________________\n",
            "cnn_layer_4 (ConvLSTM2D)     (None, 8, 28, 14, 10)     7240      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 8, 14, 7, 10)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 14, 7, 10)      40        \n",
            "_________________________________________________________________\n",
            "cnn_layer_5 (ConvLSTM2D)     (None, 8, 12, 5, 10)      7240      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3 (None, 8, 6, 2, 10)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 6, 2, 10)       40        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               123008    \n",
            "=================================================================\n",
            "Total params: 156,888\n",
            "Trainable params: 156,788\n",
            "Non-trainable params: 100\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "2241/8784 [======>.......................] - ETA: 2:34:56 - loss: 3.2546 - accuracy: 0.0992"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}